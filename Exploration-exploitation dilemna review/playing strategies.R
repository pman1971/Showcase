### CLEAN WORKSPACE AND SOURCE FUNCTIONS ####
# Clear all objects
rm(list=ls())

# Clear all plots
dev.off()

######################################################################################################
### “Should I go for the decision that seems to be optimal, assuming that my current knowledge 
### is reliable enough? Or should I go for a decision that seems to be sub-optimal for now, 
### making the assumption that my knowledge could be inaccurate and that gathering new information 
### could help me to improve it?”. In very simple terms, this is how we could summarise the 
### exploration-exploitation trade-off (also known as exploration-exploitation dilemma). 
### The ideas of exploration and exploitation are central to designing an expedient reinforcement 
### learning system. Below are a few examples of different strategies
######################################################################################################

# Generate 3 random probabilities
probs.random<- runif(3)
probs.random<- c(0.4, 0.5, 0.6)

# Plot 3 probs
barplot(probs.random, main= max(probs.random))

# Define number of plays to find these underlying probabilities
plays= 300

set.seed(5)

### 1.MAXIMUM ###
# Draw from known maximum probabitity 10000 times 
max.prob= max(probs.random)

wins.max= sample(c(1, 0), size= plays, replace= TRUE, prob = c(max.prob, 1-max.prob))

### 2.PLAY EACH ACTION EQUALLY ###
wins1= sample(c(1, 0), size= plays/3, replace= TRUE, prob = c(probs.random[1], 1-probs.random[1]))
wins2= sample(c(1, 0), size= plays/3, replace= TRUE, prob = c(probs.random[2], 1-probs.random[2]))
wins3= sample(c(1, 0), size= plays/3, replace= TRUE, prob = c(probs.random[3], 1-probs.random[3]))

wins.equal= sum(c(wins1, wins2, wins3))

### 3.PLAY RANDOMLY ###

play.cuml=0
for(i in 1:plays)
{
  prob.play= sample(1:3, 1)
  play= sample(c(1,0), size= 1, 
               prob= c(probs.random[prob.play], 1-probs.random[prob.play]))
  play.cuml= play.cuml+play
}

wins.random= play.cuml

### 4.TEST EACH ONE FOR A SMALL SAMPLE ###
# Play each bandit a number of times and then choose the best one
max.plays= 30

wins1= sample(c(1, 0), size= max.plays, replace= TRUE, prob = c(probs.random[1], 1-probs.random[1]))
wins2= sample(c(1, 0), size= max.plays, replace= TRUE, prob = c(probs.random[2], 1-probs.random[2]))
wins3= sample(c(1, 0), size= max.plays, replace= TRUE, prob = c(probs.random[3], 1-probs.random[3]))
wins.sum= c(sum(wins1), sum(wins2), sum(wins3))

prob.max= which.max(wins.sum)

# Play best
wins.bestprob= sample(c(1, 0), size= plays- 3*max.plays, replace= TRUE, prob = c(probs.random[prob.max], 1-probs.random[prob.max]))
wins.best= sum(c(wins1, wins2, wins3, wins.bestprob))

### 5.EPSILON GREEDY ###
# As you play the machines, you keep track of the average payout of each machine.
# Then, you select the machine with the highest current average payout with
# probability = (1 – epsilon) + (epsilon / k) where epsilon is a small value like 0.10.
# And you select machines that don’t have the highest current payout average
# with probability = epsilon / k.
win1=0
win2=0
win3=0
loss1=0
loss2=0
loss3=0
epsilon = 0.1
k = length(probs.random)
results = c(0, 0, 0)
for(i in 1:plays)
{
  # Choose random prob
  epsilon.prob = runif(1)
  # All equal or epsilon exploration
  if(min(results) == max(results) | epsilon.prob <= epsilon)
  {
    prob.play= sample(1:3, 1, prob= c(1/k, 1/k, 1/k))
  }
  if(epsilon.prob > epsilon)
  {
    prob.play= which.max(results)
  }
  play= sample(c(1,0), size= 1, prob= c(probs.random[prob.play], 1-probs.random[prob.play]))
  # Update results
  if(prob.play== 1)
  {
    win1= win1+play
    loss1= loss1+(1-play)
  }
  if(prob.play== 2)
  {
    win2= win2+play
    loss2= loss2+(1-play)
  }
  if(prob.play== 3)
  {
    win3= win3+play
    loss3= loss3+(1-play)
  }
  results = c(win1/(win1+loss1), win2/(win2+loss2), win3/(win3+loss3))
  results[is.na(results)] = 0
}

wins.epsilon= sum(win1, win2, win3)

#### THOMPSON SAMPLING ###

win1=0
win2=0
win3=0
loss1=0
loss2=0
loss3=0

for(i in 1:plays)
{
  # Draw random sample from known (posterior???) distribution
  # Without any prior information assume uniform distribution
  beta.prob1= rbeta(1, 1+win1, 1+loss1)
  beta.prob2= rbeta(1, 1+win2, 1+loss2)
  beta.prob3= rbeta(1, 1+win3, 1+loss3)
  beta.probs= c(beta.prob1, beta.prob2, beta.prob3)
  
  max.beta.prob= which.max(beta.probs)
  
  # Play highest drawn probability
  play= sample(c(1,0), size= 1, 
               prob= c(probs.random[max.beta.prob], 1-probs.random[max.beta.prob]))
  
  # Update played beta distribution
  if(max.beta.prob== 1)
  {
    win1= win1+play
    loss1= loss1+(1-play)
  }
  if(max.beta.prob== 2)
  {
    win2= win2+play
    loss2= loss2+(1-play)
  }
  if(max.beta.prob== 3)
  {
    win3= win3+play
    loss3= loss3+(1-play)
  }
}

wins.thomsamp= sum(win1, win2, win3)

# Review of strategies
par(mar = c(3, 9, 2, 2))
barplot(c(sum(wins.max), wins.equal, wins.random, wins.best, wins.epsilon, wins.thomsamp), xlim= c(0, sum(wins.max*1.1)), 
        main= paste("No of plays-", plays), 
        horiz = T,
        las= 1,
        names.arg= c('Best known', 'Play equally', 'Play randomly', 'Test sample', 'Epsilon greedy', 'Thompson sampling')) 


